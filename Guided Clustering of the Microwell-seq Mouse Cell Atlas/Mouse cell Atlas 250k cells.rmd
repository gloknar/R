---
title: "Tutorial eficiencia Seurat V3 MCA 250k"
author: "Adam Casas"
date: 'Compilado: `r format(Sys.Date(), "%d de %B del %Y")`'
output: 
  html_document:
    df_print: paged
    highlight: kate
    number_sections: yes
    theme: cerulean
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  tidy = TRUE
)
```

***

Adaptado de la guía publicada por el [laboratorio Satija](https://satijalab.org/seurat/v3.2/mca.html). 

# Introducción al tutorial

En este tutorial vamos a mostrar un workflow para manejar datasets muy grandes, haciendo hincapié en el uso eficiente de la memoria y el tiempo de CPU de nuestro sistema. Por tanto, no analizaremos el sentido biológico de los clusters de células resultantes del workflow, aunque si te sientes preparado, te invitamos a que los analices por tu cuenta. Los análisis que realicemos en este tutorial se harán en la memoria RAM, aunque Seurat ha añadido recientemente soporte para realizar el análisis desde el disco duro mediante el framework [loom](http://loompy.org/). En esta [viñeta](https://satijalab.org/seurat/mca_loom.html) puedes ver este mismo workflow, pero usando [loomR](https://satijalab.org/loomR/loomR_tutorial.html).


***

<br>

# Workflow: Creamos el objeto Seurat `mca`

El dataset MCA (Mouse Cell Atlas) fue generado por [Xiaoping y compañeros, 2018](https://www.sciencedirect.com/science/article/pii/S0092867418301168), contiene 250000 células y está disponible en este [enlace](https://figshare.com/articles/dataset/MCA_DGE_Data/5435866). No obstante, para agilizar el workflow partimos directamente de un archivo `.rds` que contiene la matriz de expresión (=matriz de conteos UMI) y los metadatos ya combinados (archivo disponible en este [enlace](https://www.dropbox.com/s/8d8t4od38oojs6i/MCA.zip?dl=1))

```{r}
library(Seurat)
library(patchwork)

matriz.conteo.UMIs <- readRDS(file = "./MCA_merged_mat.rds")
metadatos <- read.csv(file = "./MCA_All-batch-removed-assignments.csv", row.names = 1)
```




Vamos a analizar las 242000 células a las que se les asignó un clúster y fueron anotadas en el estudio original del que provienen los datos. Por tanto no hace falta hacer el control de calidad (cosa que sí hicimos en el tutorial de las 2700 PBMCs).

Nótese que en la matriz de conteos hay 405192 células (=columnas), pero en los metadatos hay menos células, ya que no todas fueron asignadas a un clúster y anotadas. Por tanto vamos a quedarnos sólo con aquellas células que hayan sido aisgnadas a un clúster con el comando `subset` del paquete `Seurat`.

```{r}
mca <- CreateSeuratObject(counts = matriz.conteo.UMIs,
                          project = "Mouse-Cell-Atlas",
                          meta.data = metadatos,
                          assay = "RNA")

mca <- subset(x = mca, cells = which(!is.na(mca$ClusterID)))
mca
```

***

<br>

## Preprocesado de datos

Normalizamos los datos con el método estándar `LogNormalize`, tal como hicimos en el tutorial de 2700 PBMCs.

```{r}
mca <- NormalizeData(mca,
                     assay = "RNA",
                     normalization.method = "LogNormalize", 
                     scale.factor = 10000)
```


El comando `FindVariableGenes` calcula la varianza y la media de cada gen del dataset y guarda los genes más variables en `mca[["RNA]]@meta.features`). Para datasets grandes que usen matrices de conteos de UMIs, seleccionar genes muy variables basándose solo en el ratio varianza-media (VMR en inglés) es una estrategia eficiente y robusta. En este caso seleccionamos los 1000 genes con mayor varianza para posteriores análisis.

```{r}
mca <- FindVariableFeatures(mca, 
                            assay = "RNA",
                            selection.method = "vst", 
                            nfeatures = 1000)
```

Ahora calculamos y controlamos la expresión de genes mitocondriales por célula para evitar que actúe como un factor de confusión o *confounder*.


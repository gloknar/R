---
title: "Tutorial eficiencia Seurat V3 MCA 250k"
author: "Adam Casas"
date: 'Compilado: `r format(Sys.Date(), "%d de %B del %Y")`'
output: 
  html_document:
    df_print: paged
    highlight: kate
    number_sections: yes
    theme: cerulean
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = T,
  warning = F,
  tidy = TRUE
)
```

***

Adaptado de la guía publicada por el [laboratorio Satija](https://satijalab.org/seurat/v3.2/mca.html). 

# Introducción al tutorial

En este tutorial vamos a mostrar un workflow para manejar datasets muy grandes, haciendo hincapié en el uso eficiente de la memoria y el tiempo de CPU de nuestro sistema. Por tanto, no analizaremos el sentido biológico de los clusters de células resultantes del workflow, aunque si te sientes preparado, te invitamos a que los analices por tu cuenta. Los análisis que realicemos en este tutorial se harán en la memoria RAM, aunque Seurat ha añadido recientemente soporte para realizar el análisis desde el disco duro mediante el framework [loom](http://loompy.org/). En esta [viñeta](https://satijalab.org/seurat/mca_loom.html) puedes ver este mismo workflow, pero usando [loomR](https://satijalab.org/loomR/loomR_tutorial.html).


***

<br>

# Workflow: Creamos el objeto Seurat `mca`

El dataset MCA (Mouse Cell Atlas) fue generado por [Xiaoping y compañeros, 2018](https://www.sciencedirect.com/science/article/pii/S0092867418301168), contiene 250000 células y está disponible en este [enlace](https://figshare.com/articles/dataset/MCA_DGE_Data/5435866). No obstante, para agilizar el workflow partimos directamente de un archivo `.rds` que contiene la matriz de expresión (=matriz de conteos UMI) y los metadatos ya combinados (archivo disponible en este [enlace](https://www.dropbox.com/s/8d8t4od38oojs6i/MCA.zip?dl=1))

```{r}
library(Seurat)
library(patchwork)
library(ggplot2)

matriz.conteo.UMIs <- readRDS(file = "./MCA_merged_mat.rds")
metadatos <- read.csv(file = "./MCA_All-batch-removed-assignments.csv", row.names = 1)
```




Vamos a analizar las 242000 células a las que se les asignó un clúster y fueron anotadas en el estudio original del que provienen los datos. Por tanto no hace falta hacer el control de calidad (cosa que sí hicimos en el tutorial de las 2700 PBMCs).

Nótese que en la matriz de conteos hay 405192 células (=columnas), pero en los metadatos hay menos células, ya que no todas fueron asignadas a un clúster y anotadas. Por tanto vamos a quedarnos sólo con aquellas células que hayan sido aisgnadas a un clúster con el comando `subset` del paquete `Seurat`.

```{r}
mca <- CreateSeuratObject(counts = matriz.conteo.UMIs,
                          project = "Mouse-Cell-Atlas",
                          meta.data = metadatos,
                          assay = "RNA")

mca <- subset(x = mca, cells = which(!is.na(mca$ClusterID)))
mca
```

***

<br>



## Preprocesado de datos

Los siguientes pasos se benefician de ser paralelizados en cuanto a tiempo de CPU se refiere, pero al estar trabajando con un dataset muy pesado y teniendo en cuenta que los requerimientos de RAM escalan con el nº de hilos que usemos, se vuelve necesario tener un sistema con mucha memoria RAM para poder paralelizar este workflow. Dado que mi ordenador de trabajo cuenta sólo con 16 GB, no usaré el siguiente código, pero siéntete libre de usarlo si cuentas con más RAM.

```{r, eval = F}
library(future)
plan(strategy = "multisession", workers = 4)

# Establecemos el límite de tamaño que puede usar `future` en 2 GBs dado que de
# lo contrario superamos el límite por defecto de 500 MBs al trabajar con
# objetos tan grandes
options(future.globals.maxSize = 2 * 1024^3)
```

<br>

### Normalización

Normalizamos los datos con el método estándar `LogNormalize`, tal como hicimos en el tutorial de 2700 PBMCs.

```{r}
mca <- NormalizeData(mca,
                     assay = "RNA",
                     normalization.method = "LogNormalize", 
                     scale.factor = 10000)
```

<br>

### Detección de genes diferencialmente expresados

El comando `FindVariableGenes` calcula la varianza y la media de cada gen del dataset y guarda los genes más variables en `mca[["RNA]]@meta.features`). Para datasets grandes que usen matrices de conteos de UMIs, seleccionar genes muy variables basándose solo en el ratio varianza-media (VMR en inglés) es una estrategia eficiente y robusta. En este caso seleccionamos los 1000 genes con mayor varianza para posteriores análisis.

```{r}
mca <- FindVariableFeatures(mca, 
                            assay = "RNA",
                            selection.method = "vst", 
                            nfeatures = 1000)
```
<br>

### Escalado y control de contaminación mitocondrial

Ahora calculamos la expresión de genes mitocondriales de las células con el comando `PercentageFeatureSet`, y con la función `ScaleData` escalamos los genes diferencialmente expresados (opción por defecto) y controlamos la contaminación mitocondrial para evitar que esta actúe como un factor de confusión o *confounder*.

```{r}
mca[["percent.mt"]] <- PercentageFeatureSet(mca, pattern = "^mt-")
mca <- ScaleData(mca, vars.to.regress = "percent.mt")
```

> Nota: Para datasets grandes, procura paralelizar el workflow de Seurat con el paquete `future` (Si tu capacidad RAM te lo permite) y escalar sólo los genes variables. Como ya se comentó en el tutorial de 2700 PBMCs, sólo los genes diferencialmente expresados se usan como input del PCA, por lo que no nos reporta ningún beneficio escalar el resto de genes. 

***

<br>

## Reducción de la dimensionalidad (PCA)


```{r}
mca <- RunPCA(mca,
              assay = "RNA",
              npcs = 100,
              nfeatures.print = 5,
              ndims.print = 1:5)

# print(mca@reductions$pca, dims = 1:5, nfeatures = 5) # = mca[["pca"]]
```

Para elegir la dimensionalidad del dataset, se podría realizar un Jackstraw, pero este dataset en concreto es muy grande, por lo que tal método queda descartado. En su lugar, podemos recurrir al elbow plot.

```{r}
ElbowPlot(mca, ndims = 100, reduction = "pca") + geom_hline(yintercept = 1, col = "red")
```

Podríamos quedarnos con todas las componentes principales que tengan una desviación estándar >=1 (línea roja), lo que nos dejaría con unas 60 componentes principales. No obstante, el laboratorio Satija decidió quedarse con 75, y como almacenar más componentes de las necesarias no afecta negativamente a los análisis, elegimos quedarnos con dichas 75 dimensiones.

```{r}
DimHeatmap(mca, dims = c(1:3, 70:75), cells = 500, reduction = "pca", balanced = T)
```
***

<br>

## Clustering

Ahora agrupamos las células mediante la técnica de clustering basadas en grafos de Seurat, ya descrita en el tutorial de 2700 PBMCs. Para el comando `FindNeighbors` usamos tantas dimensiones como las que hayamos conservado en el PCA (75), y el parámetro `nn.eps` vamos a establecerlo en $$0.5$$. El valor por defecto de `nn.eps` es 0 (busca vecinos exactos), pero para datasets grandes, se aconseja incrementar este valor (busca vecinos aproximados), ya que así conseguimos acelerar nuestro código.


```{r}
mca <- FindNeighbors(mca, 
                     reduction = "pca", 
                     dims = 1:75,
                     assay = "RNA",
                     nn.eps = 0.5)  # `nn.eps = 0` sería más lento 

mca <- FindClusters(mca, resolution = 3) # resolution > 1.2 ya que el dataset tiene > 3000 células
```



***

<br>

## Visualización tSNE/UMAP


ee





















e